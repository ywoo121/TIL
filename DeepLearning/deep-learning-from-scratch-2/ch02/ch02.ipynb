{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ch02. 자연어와 단어의 분산 표현\n",
    "자연어 처리? -> 컴퓨터가 우리의 말을 이해하게 만드는 것\n",
    "\n",
    "## 2.1 자연어 처리란?\n",
    "자연어: 한국어와 영어 등 우리가 평소에 쓰는 말\n",
    "\n",
    "\n",
    "자연어 처리 (Natural Language Processing, NLP)\n",
    "   - 목표: 사람의 말을 컴퓨터가 이해하도록 만들어서, 컴퓨터가 우리에게 도움이 되는 일 수행하도록 하는 것\n",
    "   - 자연어: '부드러운 언어' -> 문장의 뜻이 애매할 수 있다거나, 그 의미나 형태가 유연하게 바뀐다는 뜻\n",
    "   - ex. IBM 왓슨 (질의응답 시스템)\n",
    "\n",
    "### 2.1.1. 단어의 의미\n",
    "우리의 말: '문자'로 구성 -> 말의 의미 : '단어'로 구성\n",
    "* 단어: 의미의 최소 단위 => '단어의 의미'를 이해시키는 것이 중요\n",
    "\n",
    "<단어의 의미를 잘 파악하는 표현 방법>\n",
    "- 시소러스를 활용한 기법 (ch.2)\n",
    "    - thesaurus: 유의어 사전\n",
    "- 통계기반 기법 (ch.2)\n",
    "- 추론 기반 기법 (ch.3)"
   ],
   "id": "6c05e8dbccc1ae71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2 시소러스 (thesaurus)\n",
    "'단어의 의미'를 나타내는 방법?\n",
    ": 사람이 직접 *단어의 의미* 정의하는 방식\n",
    "\n",
    "=> **시소러스** 이용 (유의어 사전)\n",
    "ex. car = auto, automobile, machine, motorcar\n",
    "\n",
    "<img src=\"img/Screenshot 2024-12-08 at 4.44.27 PM.png\" width=\"40%\" height=\"30%\" title=\"px(픽셀) 크기 설정\" alt=\"단어들의 의미를 상.하위 관계에 기초에 그래프로 표현\"></img>\n",
    "\n",
    "\n",
    "-> 모든 단어들에 대한 *유의어 집합*을 만든 다음, 단어들의 관계를 *그래프로 표현*하여 단어 사이의 연결 정의\n",
    "\n",
    "-> 단어 네트워크를 이용하여 컴퓨터에게 단어 사이의 관계를 가르칠 수 있음\n",
    "\n",
    "### 2.2.1 WordNet\n",
    "**WordNet** : 자연어 처리 분야에서 가장 유명한 시소러스\n",
    "\n",
    "-> 이를 사용하면 유의어를 얻거나 '단어 네트워크' 이용 가능\n",
    "\n",
    "-> 단어 사이의 유사도 구할 수 있음\n",
    "\n",
    "### 2.2.2 시소러스의 문제점\n",
    "시소러스에는 수많은 단어에 대한 동의어와 계층 구조 등의 관계가 정의되어 있음\n",
    "\n",
    "이를 이용하면 '단어의 의미'를 컴퓨터에 전달 가능\n",
    "-> 이렇게 수작업으로 사람이 레이블링하면 결점 존재\n",
    "\n",
    "<시소러스 방식의 대표적인 문제점>\n",
    "- 시대 변화에 대응하기 어렵다 (단어 의미 변화 가능성)\n",
    "- 사람을 쓰는 비용은 크다\n",
    "- 단어의 미묘한 차이를 표현할 수 없다\n",
    "=> 이거를 해결하기 위해 '통계 기반 기법' or 신경망을 이용한 '추론 기반 기법' 사용"
   ],
   "id": "4ee36ff73cee7499"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.3 통계 기반 기법\n",
    "> **말뭉치** (NLP 연구를 염두로 두고 수집된 대량의 텍스트 데이터, corpus) 사용\n",
    "> - 자연어에 대한 사람의 '지식'이 담겨있다고 볼 수 있음\n",
    ">   ex. 문장을 쓰는 방법, 단어 선택 방법, 단어의 의미 등\n",
    "> - 통계 기반 기법의 목표: 사람의 지식으로 가득찬 말뭉치에서 자동으로, 효율적으로 그 핵심을 추출하는 것\n",
    "\n",
    "### 2.3.1 파이썬으로 말뭉치 전처리하기\n",
    "전처리: 텍스트 데이터를 단어로 분할하고, 그 분할된 단어들을 단어 ID 목록으로 변환하는 일"
   ],
   "id": "6f59187b56eaf5f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:04:59.588026Z",
     "start_time": "2024-12-08T08:04:59.583200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from distutils.msvc9compiler import query_vcvarsall\n",
    "from itertools import count\n",
    "\n",
    "text = \"You say goodbye and I say hello.\""
   ],
   "id": "cb7714adc8cae31f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:32:24.004571Z",
     "start_time": "2024-12-08T08:32:24.001671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = text.lower()\n",
    "text = text.replace(\".\", \" .\")\n",
    "text"
   ],
   "id": "cf42c377c757ec0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you say goodbye and i say hello .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "words = text.split(\" \")\n",
    "words"
   ],
   "id": "26e0363576b189ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `lower`: 모든 문자를 소문자로 변환 (대문자로 시작하는 단어도 똑같이 소문자와 취급하려고)\n",
    "- `split(' ')`: 공백을 기준으로 분할\n",
    "- `replace('.', ' . ')`: 단어를 분할하기 위해\n",
    "\n",
    "=> 단어 목록으로 분할되었지만, ID를 부여하는 과정을 거쳐야 조작하기 편리\n",
    "\n",
    "=> 파이썬의 dictionary 이용해서 단어 ID와 단어를 짝지어주는 대응표 작성"
   ],
   "id": "c4a0676a558dc6b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:13:01.940850Z",
     "start_time": "2024-12-08T08:13:01.936740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "\n",
    "print(\"id_to_word:\", id_to_word)\n",
    "print(\"word_to_id:\", word_to_id)"
   ],
   "id": "273e2d17a60d5030",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_to_word: {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.', 7: ''}\n",
      "word_to_id: {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6, '': 7}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 단어 목록 -> 단어 ID 목록\n",
    "python의 내포 (comprehension) 표기를 이용해서 단어 목록 -> ID 목록, 넘파이 배열으로 변환\n",
    "> 내포\n",
    "> ```python\n",
    "> xs = [1, 2, 3, 4]     #를 제곱하여 새로운 리스트?\n",
    "> [x**2 for x in xs]    #라고 사용하면 됨\n",
    "> ```"
   ],
   "id": "4bcfb4df01f7ed60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:18:40.354811Z",
     "start_time": "2024-12-08T08:18:40.349548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ],
   "id": "7e06ad1638af3861",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 1, 5, 6, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:32:28.601776Z",
     "start_time": "2024-12-08T08:32:28.596550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\".\", \" .\")\n",
    "    words = text.split(\" \")\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ],
   "id": "28d572f0118e8677",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:22:30.321011Z",
     "start_time": "2024-12-08T08:22:30.318532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ],
   "id": "104a9e0fbfad1f0c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "∴ 말뭉치 전처리 완료!\n",
    "\n",
    "\n",
    "### 2.3.2 단어의 분산 표현\n",
    "단어의 **분산 표현** = '단어의 의미'를 정확하게 파악할 수 있는 벡터 표현\n",
    "> 단어의 분선 표현은 단어를 고정 길이의 밀집벡터로 표현\n",
    "> 밀집벡터: 대부분의 원소가 0이 아닌 실수인 벡터를 의미\n",
    "> ex. [0.21, -0.45, 0.83]\n",
    "\n",
    "### 2.3.3 분포 가설\n",
    "단어-> 벡터 표현: '단어의 의미는 주변 단어에 의해 형성됨'\n",
    "\n",
    "==> **분포 가설**\n",
    ": 단어 자체에는 의미가 없고, 그 단어가 사용된 _'맥락'_이 의미를 형성한다.\n",
    "\n",
    "<img src=\"img/2-3.png\">\n",
    "\n",
    "- 맥락: (주목하는 단어) 주변에 놓인 단어\n",
    "- 원도우 크기: 맥락의 크기 (주변 단어를 몇 개나 포함할지)\n",
    "\n",
    "### 2.3.4 동시발생 행렬\n",
    "분포 가설에 기초에 단어를 벡터로 나타내는 방법? => 주변 단어를 세어본다.....\n",
    "\n",
    "=> 통계 기반 기법\""
   ],
   "id": "773d8caeb76837ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:32:36.818773Z",
     "start_time": "2024-12-08T08:32:36.813905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ],
   "id": "4a71b9dea901c265",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "이 문장에 있는 단어들에 대한 맥락을 세어보면?\n",
    "\n",
    "<img src=\"img/2-7.png\" width = 70%>\n",
    "\n",
    "모든 단어에 대해 동시발생하는 단어를 표에 정리한 것.\n",
    "- 각 행: 해당 단어를 표현한 벡터\n",
    "- **동시발생 행렬** : 행렬의 형태를 띄므로"
   ],
   "id": "9d620907b016055d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:37:54.296510Z",
     "start_time": "2024-12-08T08:37:54.292937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 동시발생 행렬\n",
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0]\n",
    "], dtype=np.int32)"
   ],
   "id": "d2f5f9f1cf5e273e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:38:37.846391Z",
     "start_time": "2024-12-08T08:38:37.842061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(C[0])     # ID가 0인 단어의 벡터표현\n",
    "print(C[4])     # ID가 4인 단어의 벡터표현\n",
    "print(C[word_to_id['goodbye']])     # 'goodbye'의 벡터 표현"
   ],
   "id": "d64f425a20c249fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0 1 0 1 0 0 0]\n",
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:56:18.275925Z",
     "start_time": "2024-12-08T08:56:18.268920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 말뭉치로부터 동시발생 행렬을 만드는 함수\n",
    "def create_to_matrix(corpus, vocab_size, window_size = 1):\n",
    "    corpus = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - 1\n",
    "            right_idx = idx + 1\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ],
   "id": "e10b580c37e4086",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3.5 벡터 간 유사도\n",
    ": 단터 벡터의 유사도 => 코사인 유사도\n",
    "\n",
    "<img src=\"img/math2-1.png\">\n",
    "\n",
    "- 분자: 벡터의 내적\n",
    "- 분모: 벡터의 노름 (벡터의 크기)\n",
    "    - L2 노름 사용 (벡터의 각 원소를 제곱해 더한 후, 다시 제곱근을 구해 계산)\n",
    "\n",
    "=> 벡터를 정규화하고 내적을 구하는 것\n"
   ],
   "id": "6167c974804f60db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T08:56:57.229746Z",
     "start_time": "2024-12-08T08:56:57.227694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 코사인 유사도 구현\n",
    "def cos_similarity(x, y):\n",
    "    nx = x / np.sqrt(np.sum(x ** 2))\n",
    "    ny = y / np.sqrt(np.sum(y ** 2))"
   ],
   "id": "5e998f15f902686c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "벡터 x, y를 정규화한 후 두 벡터의 내적을 구한 것.\n",
    "\n",
    "-> 인수로 제로 벡터가 들어오면 '0 나누기' 문제 발생.\n",
    "\n",
    "=> 해결) 나눌 때 분모에 작은 값을 더해주는 것 (eps, 기본값 : 1e-8)"
   ],
   "id": "2600c0e7a28a1486"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:02:24.543792Z",
     "start_time": "2024-12-08T09:02:24.540263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(x**2)) + eps)"
   ],
   "id": "682057b545ff6865",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:27:40.369222Z",
     "start_time": "2024-12-08T09:27:40.363027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "print(cos_similarity(c0, c1))"
   ],
   "id": "c20226ac0dc05792",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "코사인 유사도 : 0.70... -> -1 ~ 1 사이이므로 상당히 _유사도가 높다_ 라는 의미\n",
    "\n",
    "### 2.3.6 유사 단어의 랭킹 표시\n",
    ": 어떤 단어가 검색어로 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수"
   ],
   "id": "57c61e7c9ddb8833"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    # 1. 검색어를 꺼낸다\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 2. 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 3. 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ],
   "id": "cc10c448728309b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 검색어의 단어 벡터를 꺼낸다\n",
    "2. 검색어의 단어 벡터와 다른 모든 단어 벡터와의 코사인 유사도를 각각 구한다.\n",
    "3. 계산한 코사인 유사도 결과를 기준으로 값이 높은 순서대로 출력한다.\n",
    "    similarity 배열에 담긴 원소의 인덱스를 내림차순으로 정렬한 후 상위 원소들을 출력. `argsort()`는  np배열의 원소를 오름차순으로 정렬."
   ],
   "id": "bb0e218a8a4a62c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T09:40:20.203611Z",
     "start_time": "2024-12-08T09:40:20.197752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.util import preprocess, create_co_matrix, most_similar\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ],
   "id": "33c285cadcc914e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.4 통계 기반 기법 개선하기\n",
    "### 2.4.1 상호정보량\n",
    "- 동시발생 행렬의 원소: 두 단어가 동시에 발생한 횟수 (but.. 별로 좋은 특징 X)\n",
    "    - the, car // car, drive: 둘 중 the, car의 동시발생 횟수가 더 높다. (단순히 the가 고빈도 단어여서)\n",
    "- 이 문제를 해결하기 위해 **점별 상호정보량(PMI)** 사용\n",
    "    - Pointwise Mutual Information\n",
    "    - <img src=\"img/math2-2.png\">\n",
    "        - P(x) : x가 일어날 확률\n",
    "        - P(y): y가 일어날 확률\n",
    "        - P(x, y): x, y가 일어날 확률\n",
    "        - PMI 값이 높을수록 관련성이 높다\n",
    "\n",
    "\n",
    "P(x) : 단어 x가 말뭉치에 등장할 확률\n",
    "<img src=\"img/math2-3.png\">\n",
    "\n",
    "- N = 10,000\n",
    "- `the`: 1000\n",
    "- `car`: 20\n",
    "- `drive`: 10\n",
    "- `the` && `car`: 10\n",
    "- `car` && `drive`: 5\n",
    "\n",
    "=> PMI 계산결과\n",
    "<img src=\"img/math2-4.png\">\n",
    "<img src=\"img/math2-5.png\">\n",
    "\n",
    "=> `car`는 `the`보다 `drive`의 관련성이 더 강해진다.\n",
    "\n",
    "단점) 두 단어의 동시발생 횟수가 0이면 log20 = -∞ 가 된다.\n",
    "\n",
    "-> 피하기 위해 실제 구현 시에는 **양의 상호정보량(PPMI)** 을 구한다\n",
    "<img src=\"img/math2-6.png\" width= \"70%\">\n"
   ],
   "id": "d483a73bb7ce852b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T10:06:49.378974Z",
     "start_time": "2024-12-08T10:06:49.372016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 동시발생 행렬을 PPMI 행렬으로 변환하는 함수\n",
    "def ppmi(C, verbose = False, eps=1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[i]*S[j])+eps)\n",
    "            W[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print(\"%.1f%% 완료\" % (100*cnt/total))\n",
    "    return M"
   ],
   "id": "15e1504a5625ea06",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- C: 동시발생 행렬\n",
    "- verbose: 진행상활 출력 여부를 결정하는 플래그"
   ],
   "id": "ccbe0c4e26ea56bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T10:09:57.187874Z",
     "start_time": "2024-12-08T10:09:57.181692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print('동시발생 행렬')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ],
   "id": "2bbe40309d21dc4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 행렬\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<PPMI 행렬의 문제점>\n",
    "- 어휘 수가 증가함에 따라 각 단어의 벡터 차원 수도 증가함.\n",
    "- 벡터는 노이즈에 약하고 견고하지 못함.\n",
    "\n",
    "\n",
    "### 2.4.2 차원 감소\n",
    ": 문자 그대로 벡터의 차원을 줄이는 방법\n",
    "\n",
    "- 단순히 줄이기만 하는 게 아니라, '중요한 정보'는 최대한 유지하면서 줄이는 게 핵심\n",
    "- 데이터의 분포를 고려해 **중요한 '축'** 을 찾는 일 수행\n",
    "- <img src=\"img/2-8.png\">\n",
    "- 1차원 값만으로도 데이터의 본질적인 차이를 구별할 수 있어야 함.\n",
    "- 특잇값 분해 (Singular Value Decomposition, SVD) 이용\n",
    "\n",
    "\n",
    "[SVD]\n",
    "- <img src=\"img/math2-7.png\">\n",
    "- U, V = 직교행렬, 그 열벡터는 서로 직교함\n",
    "- S = 대각행렬 (대각성분 외에는 모두 0)\n",
    "- <img src=\"img/2-9.png\">\n",
    "\n",
    "    - U: 직교행렬 -> '단어 공간'으로 취급 가능\n",
    "    - S: 대각행렬. '특이값(해당 축의 중요도)'이 큰 순서대로 나열되어 있음\n"
   ],
   "id": "ffd15c1df028bb91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4.3 SVD에 의한 차원 감소\n",
    "- SVD: numpy의 Linalg 모듈이 제공하는 svd 메소드로 실행 가능\n",
    "\n"
   ],
   "id": "7246e226c845d662"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.util import preprocess, create_co_matrix, ppmi\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)"
   ],
   "id": "c99e91baf43fb3a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4.4 PTB 데이터셋\n",
    "PTB (펜트리뱅크, Penn Treebank): '본격적인' 말뭉치\n",
    "- 한 문장이 하나의 줄로 저장\n",
    "- 'eos': end of sentence"
   ],
   "id": "3fc9df57acc09a79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T12:23:59.114862Z",
     "start_time": "2024-12-08T12:23:57.865782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print('말뭉치 크기: ', len(corpus))\n",
    "print('corpus[:30]', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']: \", word_to_id['car'])\n",
    "print(\"word_to_id['happy']: \", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']: \", word_to_id['lexus'])"
   ],
   "id": "e5e4226f8dc0bc4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.train.txt ... \n",
      "Done\n",
      "말뭉치 크기:  929589\n",
      "corpus[:30] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']:  3856\n",
      "word_to_id['happy']:  4428\n",
      "word_to_id['lexus']:  7426\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `corpus`: 단어 ID 목록\n",
    "- `id_to_word`: 단어 ID -> 단어로 변환하는 딕셔너리\n",
    "- `word_to_id`: 단어 -> 단어 ID\n",
    "\n",
    "### 2.4.5 PTB 데이터셋 평가"
   ],
   "id": "5a4c5690f6f16028"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T12:45:45.726763Z",
     "start_time": "2024-12-08T12:36:21.647608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from dataset import ptb\n",
    "from common.util import most_similar, ppmi, create_co_matrix\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('동시발생 수 계산...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=window_size)\n",
    "print('PPMI 계산...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('SVD 계산...')\n",
    "try:\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "    print('### calculating SVD using sklearn ###')\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n",
    "\n",
    "except ImportError:\n",
    "    print('### calculating SVD using normal linear algebra ###')\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ],
   "id": "52aef86995ba31dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 수 계산...\n",
      "PPMI 계산...\n",
      "1.0% 완료\n",
      "2.0% 완료\n",
      "3.0% 완료\n",
      "4.0% 완료\n",
      "5.0% 완료\n",
      "6.0% 완료\n",
      "7.0% 완료\n",
      "8.0% 완료\n",
      "9.0% 완료\n",
      "10.0% 완료\n",
      "11.0% 완료\n",
      "12.0% 완료\n",
      "13.0% 완료\n",
      "14.0% 완료\n",
      "15.0% 완료\n",
      "16.0% 완료\n",
      "17.0% 완료\n",
      "18.0% 완료\n",
      "19.0% 완료\n",
      "20.0% 완료\n",
      "21.0% 완료\n",
      "22.0% 완료\n",
      "23.0% 완료\n",
      "24.0% 완료\n",
      "25.0% 완료\n",
      "26.0% 완료\n",
      "27.0% 완료\n",
      "28.0% 완료\n",
      "29.0% 완료\n",
      "30.0% 완료\n",
      "31.0% 완료\n",
      "32.0% 완료\n",
      "33.0% 완료\n",
      "34.0% 완료\n",
      "35.0% 완료\n",
      "36.0% 완료\n",
      "37.0% 완료\n",
      "38.0% 완료\n",
      "39.0% 완료\n",
      "40.0% 완료\n",
      "41.0% 완료\n",
      "42.0% 완료\n",
      "43.0% 완료\n",
      "44.0% 완료\n",
      "45.0% 완료\n",
      "46.0% 완료\n",
      "47.0% 완료\n",
      "48.0% 완료\n",
      "49.0% 완료\n",
      "50.0% 완료\n",
      "51.0% 완료\n",
      "52.0% 완료\n",
      "53.0% 완료\n",
      "54.0% 완료\n",
      "55.0% 완료\n",
      "56.0% 완료\n",
      "57.0% 완료\n",
      "58.0% 완료\n",
      "59.0% 완료\n",
      "60.0% 완료\n",
      "61.0% 완료\n",
      "62.0% 완료\n",
      "63.0% 완료\n",
      "64.0% 완료\n",
      "65.0% 완료\n",
      "66.0% 완료\n",
      "67.0% 완료\n",
      "68.0% 완료\n",
      "69.0% 완료\n",
      "70.0% 완료\n",
      "71.0% 완료\n",
      "72.0% 완료\n",
      "73.0% 완료\n",
      "74.0% 완료\n",
      "75.0% 완료\n",
      "76.0% 완료\n",
      "77.0% 완료\n",
      "78.0% 완료\n",
      "79.0% 완료\n",
      "80.0% 완료\n",
      "81.0% 완료\n",
      "82.0% 완료\n",
      "83.0% 완료\n",
      "84.0% 완료\n",
      "85.0% 완료\n",
      "86.0% 완료\n",
      "87.0% 완료\n",
      "88.0% 완료\n",
      "89.0% 완료\n",
      "90.0% 완료\n",
      "91.0% 완료\n",
      "92.0% 완료\n",
      "93.0% 완료\n",
      "94.0% 완료\n",
      "95.0% 완료\n",
      "96.0% 완료\n",
      "97.0% 완료\n",
      "98.0% 완료\n",
      "99.0% 완료\n",
      "SVD 계산...\n",
      "### calculating SVD using normal linear algebra ###\n",
      "\n",
      "[query] you\n",
      " i: 0.7003179788589478\n",
      " we: 0.6367185115814209\n",
      " anybody: 0.5657642483711243\n",
      " do: 0.563567042350769\n",
      " 'll: 0.5127797722816467\n",
      "\n",
      "[query] year\n",
      " month: 0.6961644291877747\n",
      " quarter: 0.6884942054748535\n",
      " earlier: 0.6663320660591125\n",
      " last: 0.6281364560127258\n",
      " next: 0.6175755858421326\n",
      "\n",
      "[query] car\n",
      " luxury: 0.6728832125663757\n",
      " auto: 0.6452109813690186\n",
      " vehicle: 0.6097723245620728\n",
      " cars: 0.6032834053039551\n",
      " corsica: 0.5698372721672058\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7585657835006714\n",
      " nissan: 0.714803159236908\n",
      " motors: 0.6926157474517822\n",
      " lexus: 0.6583304405212402\n",
      " honda: 0.6350275278091431\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "=> 단어의 의미, 문법적인 관점에서 비슷한 단어들이 가까운 벡터로 나타남\n",
    "\n",
    "1. 말뭉치를 사용해서 맥락에 속한 단어의 등장 횟수를 센 후 PPMI 행렬으로 변환\n",
    "2. SVD를 이용해 차원을 감소\n",
    "3. 더 좋은 단어 벡터 얻기\n",
    "\n",
    "=> 단어의 분산 표현, 각 단어는 고정 길이의 밀집 벡터로 표현됨"
   ],
   "id": "49f7c3f362105c61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.5 정리\n",
    "<컴퓨터에게 '단어의 의미' 이해시키는 방법>\n",
    "- 시소러스\n",
    "    - 사람이 수작업으로 정의\n",
    "    - 표현력에 한계 있음\n",
    "- 통계 기반 기법\n",
    "    - 말뭉치로부터 단어의 의미를 자동으로 추출\n",
    "    - 그 의미를 벡터로 표현\n",
    "    - 단어의 동시발생 행렬을 만들고, PPMI 행렬으로 변환, SVD 이용해 차원 감소시켜 각 단어의 분산 표현 생성\n",
    "    - 그 분산 표현에 따르면 의미/문법적인 용법이 비슷한 단어들이 벡터 공간에서도 서로 가까이 모여 있음을 확인 가능\n"
   ],
   "id": "fe3bba81514e1fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 이번 장에서 배운 내용\n",
    "- WordNet 등의 시소러스를 이용하면 유의어를 얻거나 단어 사이의 유사도를 측정하는 등 유용한 작업을 할 수 있다.\n",
    "- 시소러스 기반 작업은 시소러스를 작성하는데 엄청난 인전 자원이 든다거나 새로운 단어에 대응하기 어렵다는 문제가 있다.\n",
    "- 현재는 말뭉치를 이용해 단어를 벡터화하는 방식이 자주 쓰인다.\n",
    "- 최근의 단어 벡터화 기법들은 대부분 '단어의 의미는 주변 단어에 의해 형성된다.'는 분포 가설에 기초한다.\n",
    "- 통계기반 기법은 말뭉치 안의 각 단어에 대해서 그 단어의 주변 단어의 빈도를 집계한다.\n",
    "- 동시발생 행렬을 PPMI 행렬으로 변환하고 다시 차원을 감소시킴으로써, 거대한 '희소벡터'를 작은 '밀집벡터'로 변환할 수 있다.\n",
    "- 단어의 벡터 공간에서는 의미가 가까운 단어는 그 거리도 가까울 것으로 보인다."
   ],
   "id": "5efcd5655c6f3ecb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9a73a39554f3e902"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
