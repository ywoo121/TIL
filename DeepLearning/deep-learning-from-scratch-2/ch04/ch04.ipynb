{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ch04. word2vec 속도 개선\n",
    "목표: word2vec 속도 개선\n",
    "\n",
    "## 4.1 word2vec 개선 1️⃣\n",
    "CBOW 모델: 거대한 말뭉치를 다루게 되면 문제 발생함\n",
    "- 입력층의 원핫\n",
    "    - 어휘 수가 많아지면 원핫 표현의 벡터 크기도 커짐\n",
    "    - 해결) `Embedding` 계층 도입\n",
    "- 은닉층 이후의 계산\n",
    "    - 해결)  `Negative Sampling` 이라는 새로운 손실함수 도입\n",
    "  \n",
    "\n",
    "### 4.1.1 Embedding 계층\n",
    " `Embedding 계층`: 가중치 매개변수로부터 '단어 ID'에 해당하는 행(벡터)를 추출하는 계층\n",
    "\n",
    "\n",
    "### 4.1.2 Embedding 계층 구현"
   ],
   "id": "667e299b1cb209f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T12:01:00.925578Z",
     "start_time": "2024-12-10T12:01:00.884137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        # or\n",
    "        # for i, word_id in enumerate(self.idx):\n",
    "        #     dW[word_id] += dout[i]\n",
    "\n",
    "        return None\n"
   ],
   "id": "db1b81da295657b2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.2 word2vec 개선 2️⃣\n",
    "은닉층 이후의 처리..\n",
    "-> 병목 어떻게 개선? **네거티브 샘플링** 을 이용해서\n",
    "\n",
    "\n",
    "### 4.2.1 은닉층 이후 계산의 문제점\n",
    "- 은닉층의 뉴런과 가중치 행렬(W_out)의 곱\n",
    "- Softmax 계층의 계산\n",
    "\n",
    "=> 어휘 수에 비례해 계산 증가\n",
    "\n",
    "### 4.2.2 다중 분류에서 이진 분류로\n",
    "- 네거티브 샘플링 기법의 핵심\n",
    " - **이진분류** : 다중 분류를 이진 분류로 근사하는 것이 네거티브 샘플링을 이해하는데 중요함\n",
    "    - 은닉층과 출력 측의 가중치 행렬의 내적은 \"say\"에 해당하는 열만을 추출\n",
    "    - 그 추출된 벡터와 은닉층 뉴런과의 내적을 계산하면 끝\n",
    "\n",
    "\n",
    "### 4.2.3 시그모이드 함수와 교차 엔트로피 오차\n",
    "이진 분류 -> 신경망\n",
    "- 점수에 **시그모이드 함수**를 적용해 확률로 변환\n",
    "- 손실을 구할 때에는 손실 함수로 '**교차엔트로피 오차**'를 사용\n",
    "![](img/4-10.png)\n",
    "\n",
    "    - y는 신경망이 출력한 확률\n",
    "    - t: 정답 레이블\n",
    "    - y-t: 두 값의 차이\n",
    "    - 정답 레이블이 1이라면 y가 1에 가까워질수록 오차가 줄어든다.\n",
    "    - 오차가 앞 계층으로 흘러가므로, 오차가 크면 '크게' 학습하고, 작으면 '작게' 학습하게 된다."
   ],
   "id": "260e1b919334b53c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "9c72f7ca5268bc2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
