{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ch04. word2vec 속도 개선\n",
    "목표: word2vec 속도 개선\n",
    "\n",
    "## 4.1 word2vec 개선 1️⃣\n",
    "CBOW 모델: 거대한 말뭉치를 다루게 되면 문제 발생함\n",
    "- 입력층의 원핫\n",
    "    - 어휘 수가 많아지면 원핫 표현의 벡터 크기도 커짐\n",
    "    - 해결) `Embedding` 계층 도입\n",
    "- 은닉층 이후의 계산\n",
    "    - 해결)  `Negative Sampling` 이라는 새로운 손실함수 도입\n",
    "  \n",
    "\n",
    "### 4.1.1 Embedding 계층\n",
    " `Embedding 계층`: 가중치 매개변수로부터 '단어 ID'에 해당하는 행(벡터)를 추출하는 계층\n",
    "\n",
    "\n",
    "### 4.1.2 Embedding 계층 구현\n",
    "행렬에서 특정 행을 추출하기는 아주 쉬움\n",
    "W에서 특정행 추출하려면? W[2] or W[5]와 같이 명시하기만 하면 됨."
   ],
   "id": "667e299b1cb209f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T06:29:10.919520Z",
     "start_time": "2024-12-11T06:29:10.915641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "print(W)\n",
    "print(\"-------------\")\n",
    "print(\"W[2]: \\n\", W[2])\n",
    "print(\"W[5]: \\n\", W[5])"
   ],
   "id": "fb8e899eaed203d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]]\n",
      "-------------\n",
      "W[2]: \n",
      " [6 7 8]\n",
      "W[5]: \n",
      " [15 16 17]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T06:29:44.659206Z",
     "start_time": "2024-12-11T06:29:44.655602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = np.array([1,0, 3, 0])\n",
    "print(W[idx])"
   ],
   "id": "f266b5c375d0231",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  4  5]\n",
      " [ 0  1  2]\n",
      " [ 9 10 11]\n",
      " [ 0  1  2]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T06:26:32.467722Z",
     "start_time": "2024-12-11T06:26:32.396635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        # or\n",
    "        # for i, word_id in enumerate(self.idx):\n",
    "        #     dW[word_id] += dout[i]\n",
    "\n",
    "        return None\n"
   ],
   "id": "db1b81da295657b2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Embedding 계층의 역전파: dh의 각 행의 값을 dW의 해당 행에 더해줘야 함.\n",
   "id": "d3e1ccd1d85bfa6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.2 word2vec 개선 2️⃣\n",
    "은닉층 이후의 처리..\n",
    "-> 병목 어떻게 개선? **네거티브 샘플링** 을 이용해서.\n",
    "softmax 대신 네거티브 샘플링을 이용하면 어휘가 아무리 많아져도 계산량을 낮은 수준에서 일정하게 억제할 수 있음\n",
    "\n",
    "\n",
    "### 4.2.1 은닉층 이후 계산의 문제점\n",
    "- 은닉층의 뉴런과 가중치 행렬(W_out)의 곱\n",
    "- Softmax 계층의 계산\n",
    "\n",
    "=> 어휘 수에 비례해 계산 증가\n",
    "### 4.2.2 다중 분류에서 이진 분류로\n",
    "- 네거티브 샘플링 기법의 핵심\n",
    " - **이진분류** : 다중 분류를 이진 분류로 근사하는 것이 네거티브 샘플링을 이해하는데 중요함\n",
    "    - 은닉층과 출력 측의 가중치 행렬의 내적은 \"say\"에 해당하는 열만을 추출\n",
    "    - 그 추출된 벡터와 은닉층 뉴런과의 내적을 계산하면 끝\n",
    "\n",
    "\n",
    "### 4.2.3 시그모이드 함수와 교차 엔트로피 오차\n",
    "이진 분류 -> 신경망\n",
    "- 점수에 **시그모이드 함수**를 적용해 확률로 변환\n",
    "- 손실을 구할 때에는 손실 함수로 '**교차엔트로피 오차**'를 사용\n",
    "![](img/4-10.png)\n",
    "\n",
    "    - y는 신경망이 출력한 확률\n",
    "    - t: 정답 레이블\n",
    "    - y-t: 두 값의 차이\n",
    "    - 정답 레이블이 1이라면 y가 1에 가까워질수록 오차가 줄어든다.\n",
    "    - 오차가 앞 계층으로 흘러가므로, 오차가 크면 '크게' 학습하고, 작으면 '작게' 학습하게 된다."
   ],
   "id": "260e1b919334b53c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2.4 다중 분류에서 이진 분류로 (구현)\n",
    "![](img/4-11.png)\n",
    "- 맥락이 \"you\", \"goodbye\"\n",
    "- target: \"say\"\n",
    "- 입력층에서 각각에 대응하는 단어 ID의 분산 표현을 추출하기 위해 Embedding 계층 사용\n",
    "\n",
    "![](img/4-13.png)"
   ],
   "id": "bdb4d7494cd2481b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.4 word2vec 남은 주제\n",
    "### 4.4.1 word2vec을 사용한 애플리케이션의 예\n",
    "자연어 처리 분야에서 단어의 분산 표현이 중요한 이유: **전이 학습**\n",
    "- 전이학습: 한 분야에서 배운 지식을 다른 분야에도 적용하는 기법\n",
    "- 단어를 고정 길이 벡터로 변환해준다\n",
    "- 문장(단어의 흐름)도 단어의 분산 표현을 사용하여 고정 길이 벡터로 변환할 수 있다.\n",
    "    - 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구하는 것 : `bag-of-words`. 단어의 순서를 고려하지 X\n",
    "\n",
    "=> 단어 / 문장을 고정 길이 벡터로 변환할 수 있다는 점은 매우 중요 -> 머신러닝 기법을 적용할 수 있기 때문\n"
   ],
   "id": "724357a55f0219d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.4.2 단어 벡터 평가 방법\n",
    "단어의 분산 표현의 우수성을 평가하는 척도\n",
    "- 유사성 (ex. cat & animal // cat & car)\n",
    "- 유추 문제 (ex. king:queen = man: ?)\n",
    "    * 단어의 의미나 문법적인 문제를 제대로 이해하고 있는지를 (어느 정도) 측정할 수 있음.\n"
   ],
   "id": "2e33ff6a3bcb80cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.5 정리\n",
    "<word2vec 고속화>\n",
    "- 개선해야 하는 이유: 말뭉치의 어휘 수 증가에 비례해 계산량 증가\n",
    "- `Embedding 계층`\n",
    "    - 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어 ID의 행(벡터)를 추출\n",
    "- `Negative Sampling`\n",
    "    - 부정적인 예를 몇 개 샘플링하는 기법, 이를 사용하면 다중 분류를 이진 분류처럼 취급 가능\n",
    "- => 핵심) '모두' 대신 **일부** 만을 처리.\n",
    "    - 계산을 효율적으로\n",
    "\n",
    "- word2vec은 전이 학습 측면에서 특히 중요하며, 그 단어의 분산 표현은 다양한 자연어 처리 작업에 이용될 수 있다. "
   ],
   "id": "844eff6d45e53ac8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
